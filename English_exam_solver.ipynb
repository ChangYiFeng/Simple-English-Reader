{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English_exam_solver.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "svHDNpUZf03z"
      },
      "source": [
        "# input txt file\n",
        "ClozeTest = '109choices.txt'\n",
        "ArticleTest = '109_article.txt'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fg2svjvf16n",
        "outputId": "9db5b4bf-9591-456d-82c8-1f3f0d84ed52"
      },
      "source": [
        "! pip install -U sentence-transformers\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModelWithLMHead, pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: sentence-transformers in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.95)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.2)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.45)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emUCWbKDf5p2",
        "outputId": "cd42836a-144f-41ce-8b3f-9e09aee86694"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking\")\n",
        "model_cloze = AutoModelWithLMHead.from_pretrained(\"bert-large-uncased-whole-word-masking\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:762: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWmAUM0Wf8v-"
      },
      "source": [
        "# read the logic of the article\n",
        "def Read(text):\n",
        "    q = False\n",
        "    a = False\n",
        "    article = []\n",
        "    question = []\n",
        "    answer = []\n",
        "    ques_each_arcticle = []\n",
        "\n",
        "    for i in range(len(text)):\n",
        "        if text[i][0:7]=='Article':\n",
        "            ques_each_arcticle.append(0)\n",
        "            text[i] = text[i].replace(('\\n'), ' ').replace(('\\t'), '')\n",
        "            article.append(text[i])\n",
        "            q = False\n",
        "            a = False\n",
        "        elif text[i][0] == 'Q' and str.isdigit(text[i][1]):\n",
        "            if len(ques_each_arcticle) != 0:\n",
        "              ques_each_arcticle[-1]+=1\n",
        "            text[i] = text[i].replace(('\\n'), ' ').replace(('\\t'), '')\n",
        "            question.append(text[i])\n",
        "            q = True\n",
        "            a = False\n",
        "        elif text[i][0] == 'A' and str.isdigit(text[i][1]):\n",
        "            text[i] = text[i].replace(('\\n'), ' ').replace(('\\t'), '')\n",
        "            answer.append(text[i])\n",
        "            q = False\n",
        "            a = True\n",
        "        elif q is True and not(text[i][0] == 'Q' and str.isdigit(text[i][1])):\n",
        "            text[i] = text[i].replace(('\\n'), ' ').replace(('\\t'), '')\n",
        "            question.append(text[i])\n",
        "            question[-2 :] = [''.join(question[-2 :])]\n",
        "        elif a is True and not(text[i][0] == 'A' and str.isdigit(text[i][1])):\n",
        "            text[i] = text[i].replace(('\\n'), ' ').replace(('\\t'), '')\n",
        "            answer.append(text[i])\n",
        "            answer[-2 :] = [''.join(answer[-2 :])]\n",
        "        else:\n",
        "            text[i] = text[i].replace(('\\n'), ' ').replace(('\\t'), '')\n",
        "            article.append(text[i])\n",
        "            article[-2 :] = [''.join(article[-2 :])]\n",
        "    return article, question, answer, ques_each_arcticle"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_s25p0C8uid"
      },
      "source": [
        "f = open(ClozeTest, 'r')\n",
        "text = f.readlines()\n",
        "q = False\n",
        "a = False\n",
        "question = []\n",
        "answer = []\n",
        "# transform the text to list\n",
        "_, question, answer, _= Read(text)\n",
        "f.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aQPF7oXf9cG"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def input_txt(Q,i, model):\n",
        "  f = Q[i]\n",
        "  F = f.replace('$', '{}')\n",
        "  sequence = F.format(tokenizer.mask_token)\n",
        "  input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "  mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
        "  token_logits = model(input)[0]\n",
        "  mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "  top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "  return top_5_tokens\n",
        "\n",
        "def choice(A, Num):\n",
        "  f = A[Num]\n",
        "  label = ['(A)', '(B)', '(C)', '(D)']\n",
        "  array = []\n",
        "  for j in range(3):\n",
        "    array.append(A[Num][f.find(label[j]) + 4:f.find(label[j+1]) - 1])\n",
        "  array.append(A[Num][f.find(label[3])+4:])\n",
        "  return array\n",
        "\n",
        "\n",
        "def Similarity_cloze(model, top_five, answer_option):\n",
        "  voc1 = answer_option\n",
        "  voc2 = top_five\n",
        "  embeddings1 = model.encode(voc1, convert_to_tensor=True)\n",
        "  embeddings2 = model.encode(voc2, convert_to_tensor=True)\n",
        "  answer_item = ['A', 'B', 'C', 'D']\n",
        "  cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
        "  for i in range(len(voc1)):\n",
        "    for j in range(len(voc2)):\n",
        "      print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(voc1[i], voc2[j], cosine_scores[i][j]))\n",
        "    print('\\n')\n",
        "  # for i in range(len(voc1)):\n",
        "  #   print('(' + answer_item[i] + ')' + voc1[i] + ':', np.sum(cosine_scores.numpy(), axis=1)[i])\n",
        "  return cosine_scores.numpy()\n",
        "\n",
        "def Answering_ordering(cos_score, answer_option):\n",
        "  score = []\n",
        "  rank = []\n",
        "  order = []\n",
        "  voc1 = answer_option\n",
        "  for i in range(len(voc1)):\n",
        "    score.append(np.max(cos_score, axis=1)[i])\n",
        "  answer_item = ['A', 'B', 'C', 'D']\n",
        "  rank = np.argsort(score)[::-1]\n",
        "  for i in range (len(voc1)):\n",
        "    order.append(answer_item[rank[i]])\n",
        "  return order"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtPYDVSHf_nd",
        "outputId": "8a88676d-1c02-4611-f5d5-781bcebd5ec5"
      },
      "source": [
        "answer_list=[]\n",
        "\n",
        "for i in range(len(question)):\n",
        "  Answer_option = choice(answer, i)\n",
        "  Similarity_score = Similarity_cloze(SentenceTransformer('stsb-mpnet-base-v2'), tokenizer.decode(input_txt(question,i,model_cloze)).split(' '), Answer_option)\n",
        "  # print('sentence: ',question[i].replace('$', '____'))\n",
        "  answer_order = Answering_ordering(Similarity_score, Answer_option)\n",
        "  answer_list.append((question[i][1:3].replace('.',''), answer_order))\n",
        "# print(answer_list)\n",
        "with open(ClozeTest.replace('.txt', '_output') + '.txt', 'w') as f:\n",
        "  for i in range(len(answer_list)):\n",
        "    f.write('A'+answer_list[i][0]+'.')\n",
        "    for j in range(len(answer_list[i][1])):\n",
        "      f.write('('+answer_list[i][1][j]+')')\n",
        "      if(j!=len(answer_list[i][1])-1):\n",
        "        f.write('->')\n",
        "    f.write('\\n')\n",
        "# top_k_accuracy=0\n",
        "# k=1\n",
        "# cloze_answer_108 = ['A','A','D','C','D','B','B','C','B','D','A','C','D','D']\n",
        "# cloze_answer_109 = ['A','C','A','A','C','B','C','D','D','A','A','D','D']\n",
        "# print(len(answer_list), len(cloze_answer_109))\n",
        "# for i in range(len(answer_list)):\n",
        "#   for j in range(k):\n",
        "#     if answer_list[i][1][j] == cloze_answer_109[i]:\n",
        "#       top_k_accuracy+=1\n",
        "# print(top_k_accuracy)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gloves \t\t heat \t\t Score: 0.2900\n",
            "gloves \t\t gloves \t\t Score: 1.0000\n",
            "gloves \t\t towel \t\t Score: 0.4229\n",
            "gloves \t\t hose \t\t Score: 0.3219\n",
            "gloves \t\t sponge \t\t Score: 0.2364\n",
            "\n",
            "\n",
            "jacket \t\t heat \t\t Score: 0.2796\n",
            "jacket \t\t gloves \t\t Score: 0.4324\n",
            "jacket \t\t towel \t\t Score: 0.3004\n",
            "jacket \t\t hose \t\t Score: 0.1906\n",
            "jacket \t\t sponge \t\t Score: 0.2568\n",
            "\n",
            "\n",
            "ring \t\t heat \t\t Score: 0.2515\n",
            "ring \t\t gloves \t\t Score: 0.2931\n",
            "ring \t\t towel \t\t Score: 0.2026\n",
            "ring \t\t hose \t\t Score: 0.2477\n",
            "ring \t\t sponge \t\t Score: 0.2958\n",
            "\n",
            "\n",
            "socks   \t\t heat \t\t Score: 0.1611\n",
            "socks   \t\t gloves \t\t Score: 0.3216\n",
            "socks   \t\t towel \t\t Score: 0.2105\n",
            "socks   \t\t hose \t\t Score: 0.2848\n",
            "socks   \t\t sponge \t\t Score: 0.3385\n",
            "\n",
            "\n",
            "check \t\t lift \t\t Score: 0.2510\n",
            "check \t\t get \t\t Score: 0.4898\n",
            "check \t\t raise \t\t Score: 0.3327\n",
            "check \t\t take \t\t Score: 0.4284\n",
            "check \t\t move \t\t Score: 0.2235\n",
            "\n",
            "\n",
            "kick \t\t lift \t\t Score: 0.2308\n",
            "kick \t\t get \t\t Score: 0.3904\n",
            "kick \t\t raise \t\t Score: 0.3759\n",
            "kick \t\t take \t\t Score: 0.3221\n",
            "kick \t\t move \t\t Score: 0.4026\n",
            "\n",
            "\n",
            "raise \t\t lift \t\t Score: 0.4335\n",
            "raise \t\t get \t\t Score: 0.5144\n",
            "raise \t\t raise \t\t Score: 1.0000\n",
            "raise \t\t take \t\t Score: 0.5012\n",
            "raise \t\t move \t\t Score: 0.4291\n",
            "\n",
            "\n",
            "show   \t\t lift \t\t Score: 0.2762\n",
            "show   \t\t get \t\t Score: 0.4046\n",
            "show   \t\t raise \t\t Score: 0.3750\n",
            "show   \t\t take \t\t Score: 0.4777\n",
            "show   \t\t move \t\t Score: 0.2393\n",
            "\n",
            "\n",
            "Both \t\t two \t\t Score: 0.7548\n",
            "Both \t\t both \t\t Score: 1.0000\n",
            "Both \t\t all \t\t Score: 0.2607\n",
            "Both \t\t none \t\t Score: 0.1569\n",
            "Both \t\t most \t\t Score: 0.2048\n",
            "\n",
            "\n",
            "Few \t\t two \t\t Score: 0.2978\n",
            "Few \t\t both \t\t Score: 0.2841\n",
            "Few \t\t all \t\t Score: 0.2101\n",
            "Few \t\t none \t\t Score: 0.3722\n",
            "Few \t\t most \t\t Score: 0.2314\n",
            "\n",
            "\n",
            "Most \t\t two \t\t Score: 0.1619\n",
            "Most \t\t both \t\t Score: 0.2048\n",
            "Most \t\t all \t\t Score: 0.5359\n",
            "Most \t\t none \t\t Score: 0.3587\n",
            "Most \t\t most \t\t Score: 1.0000\n",
            "\n",
            "\n",
            "Some   \t\t two \t\t Score: 0.3671\n",
            "Some   \t\t both \t\t Score: 0.3655\n",
            "Some   \t\t all \t\t Score: 0.3698\n",
            "Some   \t\t none \t\t Score: 0.3265\n",
            "Some   \t\t most \t\t Score: 0.3756\n",
            "\n",
            "\n",
            "dangerous \t\t dangerous \t\t Score: 1.0000\n",
            "dangerous \t\t sad \t\t Score: 0.1829\n",
            "dangerous \t\t scary \t\t Score: 0.5157\n",
            "dangerous \t\t bad \t\t Score: 0.3609\n",
            "dangerous \t\t peaceful \t\t Score: 0.1818\n",
            "\n",
            "\n",
            "exciting \t\t dangerous \t\t Score: 0.2804\n",
            "exciting \t\t sad \t\t Score: 0.3425\n",
            "exciting \t\t scary \t\t Score: 0.5386\n",
            "exciting \t\t bad \t\t Score: 0.1087\n",
            "exciting \t\t peaceful \t\t Score: 0.3806\n",
            "\n",
            "\n",
            "lonely \t\t dangerous \t\t Score: 0.1588\n",
            "lonely \t\t sad \t\t Score: 0.5288\n",
            "lonely \t\t scary \t\t Score: 0.3278\n",
            "lonely \t\t bad \t\t Score: 0.1931\n",
            "lonely \t\t peaceful \t\t Score: 0.3547\n",
            "\n",
            "\n",
            "popular   \t\t dangerous \t\t Score: 0.2611\n",
            "popular   \t\t sad \t\t Score: 0.3753\n",
            "popular   \t\t scary \t\t Score: 0.3829\n",
            "popular   \t\t bad \t\t Score: 0.1879\n",
            "popular   \t\t peaceful \t\t Score: 0.3399\n",
            "\n",
            "\n",
            "I \t\t myself \t\t Score: 0.6872\n",
            "I \t\t himself \t\t Score: 0.2732\n",
            "I \t\t day \t\t Score: 0.2695\n",
            "I \t\t ourselves \t\t Score: 0.1917\n",
            "I \t\t yourself \t\t Score: 0.2613\n",
            "\n",
            "\n",
            "me \t\t myself \t\t Score: 0.6805\n",
            "me \t\t himself \t\t Score: 0.3383\n",
            "me \t\t day \t\t Score: 0.3186\n",
            "me \t\t ourselves \t\t Score: 0.2428\n",
            "me \t\t yourself \t\t Score: 0.2684\n",
            "\n",
            "\n",
            "myself \t\t myself \t\t Score: 1.0000\n",
            "myself \t\t himself \t\t Score: 0.4627\n",
            "myself \t\t day \t\t Score: 0.2548\n",
            "myself \t\t ourselves \t\t Score: 0.3562\n",
            "myself \t\t yourself \t\t Score: 0.5562\n",
            "\n",
            "\n",
            "mine   \t\t myself \t\t Score: 0.7217\n",
            "mine   \t\t himself \t\t Score: 0.2989\n",
            "mine   \t\t day \t\t Score: 0.3239\n",
            "mine   \t\t ourselves \t\t Score: 0.2754\n",
            "mine   \t\t yourself \t\t Score: 0.3923\n",
            "\n",
            "\n",
            "worked \t\t worked \t\t Score: 1.0000\n",
            "worked \t\t works \t\t Score: 0.6035\n",
            "worked \t\t taught \t\t Score: 0.3033\n",
            "worked \t\t lives \t\t Score: 0.1606\n",
            "worked \t\t volunteered \t\t Score: 0.2985\n",
            "\n",
            "\n",
            "has worked \t\t worked \t\t Score: 0.6504\n",
            "has worked \t\t works \t\t Score: 0.5960\n",
            "has worked \t\t taught \t\t Score: 0.2735\n",
            "has worked \t\t lives \t\t Score: 0.2627\n",
            "has worked \t\t volunteered \t\t Score: 0.2027\n",
            "\n",
            "\n",
            "is working \t\t worked \t\t Score: 0.7042\n",
            "is working \t\t works \t\t Score: 0.6710\n",
            "is working \t\t taught \t\t Score: 0.1713\n",
            "is working \t\t lives \t\t Score: 0.1897\n",
            "is working \t\t volunteered \t\t Score: 0.1643\n",
            "\n",
            "\n",
            "works   \t\t worked \t\t Score: 0.6035\n",
            "works   \t\t works \t\t Score: 1.0000\n",
            "works   \t\t taught \t\t Score: 0.2096\n",
            "works   \t\t lives \t\t Score: 0.3648\n",
            "works   \t\t volunteered \t\t Score: 0.2776\n",
            "\n",
            "\n",
            "lends \t\t saves \t\t Score: 0.3672\n",
            "lends \t\t saved \t\t Score: 0.2806\n",
            "lends \t\t gives \t\t Score: 0.4668\n",
            "lends \t\t buys \t\t Score: 0.3340\n",
            "lends \t\t save \t\t Score: 0.4049\n",
            "\n",
            "\n",
            "prepares \t\t saves \t\t Score: 0.2873\n",
            "prepares \t\t saved \t\t Score: 0.1860\n",
            "prepares \t\t gives \t\t Score: 0.2191\n",
            "prepares \t\t buys \t\t Score: 0.1976\n",
            "prepares \t\t save \t\t Score: 0.3128\n",
            "\n",
            "\n",
            "saves \t\t saves \t\t Score: 1.0000\n",
            "saves \t\t saved \t\t Score: 0.8581\n",
            "saves \t\t gives \t\t Score: 0.3327\n",
            "saves \t\t buys \t\t Score: 0.3241\n",
            "saves \t\t save \t\t Score: 0.8754\n",
            "\n",
            "\n",
            "takes   \t\t saves \t\t Score: 0.4338\n",
            "takes   \t\t saved \t\t Score: 0.2826\n",
            "takes   \t\t gives \t\t Score: 0.4764\n",
            "takes   \t\t buys \t\t Score: 0.3346\n",
            "takes   \t\t save \t\t Score: 0.4023\n",
            "\n",
            "\n",
            "is writing \t\t wrote \t\t Score: 0.7198\n",
            "is writing \t\t composed \t\t Score: 0.2867\n",
            "is writing \t\t heard \t\t Score: 0.1601\n",
            "is writing \t\t played \t\t Score: 0.0398\n",
            "is writing \t\t learned \t\t Score: 0.1201\n",
            "\n",
            "\n",
            "has written \t\t wrote \t\t Score: 0.7650\n",
            "has written \t\t composed \t\t Score: 0.3755\n",
            "has written \t\t heard \t\t Score: 0.2433\n",
            "has written \t\t played \t\t Score: 0.1322\n",
            "has written \t\t learned \t\t Score: 0.2298\n",
            "\n",
            "\n",
            "will write \t\t wrote \t\t Score: 0.6198\n",
            "will write \t\t composed \t\t Score: 0.2165\n",
            "will write \t\t heard \t\t Score: 0.1486\n",
            "will write \t\t played \t\t Score: -0.0267\n",
            "will write \t\t learned \t\t Score: 0.1583\n",
            "\n",
            "\n",
            "wrote   \t\t wrote \t\t Score: 1.0000\n",
            "wrote   \t\t composed \t\t Score: 0.4639\n",
            "wrote   \t\t heard \t\t Score: 0.3769\n",
            "wrote   \t\t played \t\t Score: 0.2751\n",
            "wrote   \t\t learned \t\t Score: 0.3758\n",
            "\n",
            "\n",
            "told \t\t knowing \t\t Score: 0.3516\n",
            "told \t\t deciding \t\t Score: 0.2024\n",
            "told \t\t explaining \t\t Score: 0.5638\n",
            "told \t\t choosing \t\t Score: 0.2785\n",
            "told \t\t asking \t\t Score: 0.3661\n",
            "\n",
            "\n",
            "to tell \t\t knowing \t\t Score: 0.3668\n",
            "to tell \t\t deciding \t\t Score: 0.1821\n",
            "to tell \t\t explaining \t\t Score: 0.4284\n",
            "to tell \t\t choosing \t\t Score: 0.2489\n",
            "to tell \t\t asking \t\t Score: 0.2558\n",
            "\n",
            "\n",
            "be told \t\t knowing \t\t Score: 0.4185\n",
            "be told \t\t deciding \t\t Score: 0.2871\n",
            "be told \t\t explaining \t\t Score: 0.5240\n",
            "be told \t\t choosing \t\t Score: 0.2705\n",
            "be told \t\t asking \t\t Score: 0.3068\n",
            "\n",
            "\n",
            "to be told   \t\t knowing \t\t Score: 0.3927\n",
            "to be told   \t\t deciding \t\t Score: 0.3599\n",
            "to be told   \t\t explaining \t\t Score: 0.4283\n",
            "to be told   \t\t choosing \t\t Score: 0.2641\n",
            "to be told   \t\t asking \t\t Score: 0.2795\n",
            "\n",
            "\n",
            "visit \t\t visit \t\t Score: 1.0000\n",
            "visit \t\t meet \t\t Score: 0.4013\n",
            "visit \t\t see \t\t Score: 0.5398\n",
            "visit \t\t join \t\t Score: 0.3536\n",
            "visit \t\t bring \t\t Score: 0.4738\n",
            "\n",
            "\n",
            "visits \t\t visit \t\t Score: 0.6746\n",
            "visits \t\t meet \t\t Score: 0.4715\n",
            "visits \t\t see \t\t Score: 0.4214\n",
            "visits \t\t join \t\t Score: 0.4580\n",
            "visits \t\t bring \t\t Score: 0.3154\n",
            "\n",
            "\n",
            "visiting \t\t visit \t\t Score: 0.9156\n",
            "visiting \t\t meet \t\t Score: 0.3014\n",
            "visiting \t\t see \t\t Score: 0.4290\n",
            "visiting \t\t join \t\t Score: 0.2823\n",
            "visiting \t\t bring \t\t Score: 0.4085\n",
            "\n",
            "\n",
            "visited   \t\t visit \t\t Score: 0.6544\n",
            "visited   \t\t meet \t\t Score: 0.4276\n",
            "visited   \t\t see \t\t Score: 0.3536\n",
            "visited   \t\t join \t\t Score: 0.4020\n",
            "visited   \t\t bring \t\t Score: 0.2376\n",
            "\n",
            "\n",
            "another \t\t a \t\t Score: 0.6300\n",
            "another \t\t another \t\t Score: 1.0000\n",
            "another \t\t the \t\t Score: 0.4996\n",
            "another \t\t this \t\t Score: 0.4927\n",
            "another \t\t one \t\t Score: 0.5729\n",
            "\n",
            "\n",
            "each \t\t a \t\t Score: 0.4916\n",
            "each \t\t another \t\t Score: 0.5759\n",
            "each \t\t the \t\t Score: 0.3414\n",
            "each \t\t this \t\t Score: 0.3410\n",
            "each \t\t one \t\t Score: 0.5183\n",
            "\n",
            "\n",
            "the next \t\t a \t\t Score: 0.3672\n",
            "the next \t\t another \t\t Score: 0.5230\n",
            "the next \t\t the \t\t Score: 0.2903\n",
            "the next \t\t this \t\t Score: 0.3394\n",
            "the next \t\t one \t\t Score: 0.2796\n",
            "\n",
            "\n",
            "the other   \t\t a \t\t Score: 0.2506\n",
            "the other   \t\t another \t\t Score: 0.5683\n",
            "the other   \t\t the \t\t Score: 0.1732\n",
            "the other   \t\t this \t\t Score: 0.2063\n",
            "the other   \t\t one \t\t Score: 0.2922\n",
            "\n",
            "\n",
            "had done \t\t did \t\t Score: 0.6614\n",
            "had done \t\t performed \t\t Score: 0.3813\n",
            "had done \t\t went \t\t Score: 0.3702\n",
            "had done \t\t worked \t\t Score: 0.4646\n",
            "had done \t\t handled \t\t Score: 0.3574\n",
            "\n",
            "\n",
            "did \t\t did \t\t Score: 1.0000\n",
            "did \t\t performed \t\t Score: 0.3972\n",
            "did \t\t went \t\t Score: 0.4591\n",
            "did \t\t worked \t\t Score: 0.4971\n",
            "did \t\t handled \t\t Score: 0.3991\n",
            "\n",
            "\n",
            "has done \t\t did \t\t Score: 0.5456\n",
            "has done \t\t performed \t\t Score: 0.3175\n",
            "has done \t\t went \t\t Score: 0.2322\n",
            "has done \t\t worked \t\t Score: 0.3737\n",
            "has done \t\t handled \t\t Score: 0.2506\n",
            "\n",
            "\n",
            "would do   \t\t did \t\t Score: 0.4483\n",
            "would do   \t\t performed \t\t Score: 0.1429\n",
            "would do   \t\t went \t\t Score: 0.1021\n",
            "would do   \t\t worked \t\t Score: 0.2044\n",
            "would do   \t\t handled \t\t Score: 0.1800\n",
            "\n",
            "\n",
            "the more \t\t the \t\t Score: 0.3131\n",
            "the more \t\t an \t\t Score: 0.3487\n",
            "the more \t\t one \t\t Score: 0.1914\n",
            "the more \t\t no \t\t Score: 0.1344\n",
            "the more \t\t another \t\t Score: 0.3573\n",
            "\n",
            "\n",
            "the most \t\t the \t\t Score: 0.2528\n",
            "the most \t\t an \t\t Score: 0.3034\n",
            "the most \t\t one \t\t Score: 0.1664\n",
            "the most \t\t no \t\t Score: 0.1629\n",
            "the most \t\t another \t\t Score: 0.2407\n",
            "\n",
            "\n",
            "the less \t\t the \t\t Score: 0.2821\n",
            "the less \t\t an \t\t Score: 0.2944\n",
            "the less \t\t one \t\t Score: 0.2452\n",
            "the less \t\t no \t\t Score: 0.3276\n",
            "the less \t\t another \t\t Score: 0.3289\n",
            "\n",
            "\n",
            "the least \t\t the \t\t Score: 0.2602\n",
            "the least \t\t an \t\t Score: 0.2696\n",
            "the least \t\t one \t\t Score: 0.2348\n",
            "the least \t\t no \t\t Score: 0.3595\n",
            "the least \t\t another \t\t Score: 0.2957\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlmANJTEgKbU"
      },
      "source": [
        "Article:Method 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvagHeYhgDWV"
      },
      "source": [
        "model_name = \"bert-large-cased-whole-word-masking-finetuned-squad\"\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer_article = AutoTokenizer.from_pretrained(model_name)\n",
        "#model to calculate cosine distance\n",
        "model_sentence = SentenceTransformer('stsb-mpnet-base-v2')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OgFJ3JLgMBV"
      },
      "source": [
        "def Similarity_Article(model_sentence, answer_from_bert, Answer_option):\n",
        "  voc1 = answer_from_bert\n",
        "  voc2 = Answer_option\n",
        "  embeddings1 = model_sentence.encode(voc1, convert_to_tensor=True)\n",
        "  embeddings2 = model_sentence.encode(voc2, convert_to_tensor=True)\n",
        "  answer_item = ['A', 'B', 'C', 'D']\n",
        "  cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2).flatten()\n",
        "  for i in range(len(voc2)):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(voc1, voc2[i], cosine_scores[i]))\n",
        "  best_answer = torch.argmax(cosine_scores)\n",
        "  print('(' + answer_item[best_answer.item()] + ')' + voc2[best_answer])\n",
        "  return cosine_scores.numpy(), answer_item[best_answer.item()]\n",
        "\n",
        "def Answering_ordering_article(cos_score, answer_option):\n",
        "  rank = []\n",
        "  order = []\n",
        "  voc1 = answer_option\n",
        "  answer_item = ['A', 'B', 'C', 'D']\n",
        "  rank = np.argsort(cos_score)[::-1]\n",
        "  for i in range (len(voc1)):\n",
        "    order.append(answer_item[rank[i]])\n",
        "  print('Output order: ',order)\n",
        "  return order"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRZ-b3zAisyH"
      },
      "source": [
        "# read article as input\n",
        "f = open(ArticleTest, 'r')\n",
        "text = f.readlines()\n",
        "q = False\n",
        "a = False\n",
        "article = []\n",
        "question = []\n",
        "answer = []\n",
        "ques_each_arcticle = []\n",
        "article, question, answer, ques_each_arcticle = Read(text)\n",
        "f.close()\n",
        "text = article"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5EPpblpgNf2",
        "outputId": "34ce01bd-e9ac-48a8-9ca2-12451b6c4bdd"
      },
      "source": [
        "# summarizer = pipeline(\"summarization\")\n",
        "answer_article_list=[]\n",
        "count = 0\n",
        "for num_article, amount in enumerate(ques_each_arcticle) :\n",
        "  for i in range(amount):\n",
        "    question[count+i]\n",
        "    inputs = tokenizer_article(question[count+i], text[num_article], add_special_tokens=True, return_tensors=\"pt\")\n",
        "    inputs_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "    outputs = qa_model(**inputs)\n",
        "    # print(summarizer(text[num_article], max_length=135, min_length=30)[0]['summary_text'])\n",
        "    Answer_option = choice(answer, count+i)\n",
        "    # answer from bert\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1\n",
        "    answer_from_bert = tokenizer_article.convert_tokens_to_string(tokenizer_article.convert_ids_to_tokens(inputs_ids[answer_start:answer_end]))\n",
        "    print(f\"Question: {question[count+i]}\")\n",
        "    print(f\"Answer: {answer_from_bert}\")\n",
        "    similarity_score_article, answer_item = Similarity_Article(model_sentence, answer_from_bert, Answer_option)\n",
        "    answer_order = Answering_ordering_article(similarity_score_article, Answer_option)\n",
        "    answer_article_list.append((question[count+i][1:3].replace('.',''), answer_order))\n",
        "  count += amount\n",
        "\n",
        "with open(ArticleTest.replace('.txt', '_output') + '.txt', 'w') as f:\n",
        "  for i in range(len(answer_article_list)):\n",
        "    f.write('A'+answer_article_list[i][0]+'.')\n",
        "    for j in range(len(answer_article_list[i][1])):\n",
        "      f.write('('+answer_article_list[i][1][j]+')')\n",
        "      if(j!=len(answer_article_list[i][1])-1):\n",
        "        f.write('->')\n",
        "    f.write('\\n')\n",
        "\n",
        "# top_k_accuracy=0\n",
        "# k=1\n",
        "# article_answer_108 = ['D','B','A','A','B','B','A','C','D','B','D','D','C','C','D']\n",
        "# article_answer_109 = ['C','C','A','D','D','B','B','B','D','A','C','B','D','D','C','D','B','C']\n",
        "\n",
        "# for i in range(len(answer_article_list)):\n",
        "#   for j in range(k):\n",
        "#     if answer_article_list[i][1][j] == article_answer_109[i]:\n",
        "#       top_k_accuracy+=1\n",
        "# print(top_k_accuracy)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: Q16. What is \"it\" in the reading? \n",
            "Answer: \n",
            " \t\t The sun. \t\t Score: -0.0362\n",
            " \t\t The rain. \t\t Score: 0.0721\n",
            " \t\t The wind. \t\t Score: 0.0898\n",
            " \t\t The rainbow.   \t\t Score: 0.1175\n",
            "(D)The rainbow.  \n",
            "Output order:  ['D', 'C', 'B', 'A']\n",
            "Question: Q17. How does Dad feel“when it comes”? \n",
            "Answer: Dad ’ s face falls\n",
            "Dad ’ s face falls \t\t He is scared. \t\t Score: 0.1860\n",
            "Dad ’ s face falls \t\t He is excited. \t\t Score: 0.1107\n",
            "Dad ’ s face falls \t\t He is not happy. \t\t Score: 0.2534\n",
            "Dad ’ s face falls \t\t He is not interested.    \t\t Score: 0.1478\n",
            "(C)He is not happy.\n",
            "Output order:  ['C', 'A', 'D', 'B']\n",
            "Question: Q18. What can we learn about Buffy’s friends? \n",
            "Answer: I made some good friends at my last yard sale.\n",
            "I made some good friends at my last yard sale. \t\t Matt has experience of yard sales. \t\t Score: 0.4553\n",
            "I made some good friends at my last yard sale. \t\t Jamie had a successful yard sale last week. \t\t Score: 0.4210\n",
            "I made some good friends at my last yard sale. \t\t Albert thinks weather is important for yard sales. \t\t Score: 0.1454\n",
            "I made some good friends at my last yard sale. \t\t Debby thinks things are cheaper online than at yard sales.   \t\t Score: 0.2860\n",
            "(A)Matt has experience of yard sales.\n",
            "Output order:  ['A', 'B', 'D', 'C']\n",
            "Question: Q19.  What do Buffy’s friends think about having a yard sale? \n",
            "Answer: Buffy : Hey, guys! I have a lot of things I don ’ t need, so I ’ m thinking about having a yard sale. What do you think?\n",
            "Buffy : Hey, guys! I have a lot of things I don ’ t need, so I ’ m thinking about having a yard sale. What do you think? \t\t Albert and Matt think it’s a nice way to make friends. \t\t Score: -0.0079\n",
            "Buffy : Hey, guys! I have a lot of things I don ’ t need, so I ’ m thinking about having a yard sale. What do you think? \t\t Debby and Albert think it’s difficult to make money from it. \t\t Score: 0.0368\n",
            "Buffy : Hey, guys! I have a lot of things I don ’ t need, so I ’ m thinking about having a yard sale. What do you think? \t\t Matt agrees with Jamie that it’s a good chance to help people. \t\t Score: 0.0995\n",
            "Buffy : Hey, guys! I have a lot of things I don ’ t need, so I ’ m thinking about having a yard sale. What do you think? \t\t Jamie agrees with Debby that it’s a lot of trouble to prepare for it.   \t\t Score: 0.0671\n",
            "(C)Matt agrees with Jamie that it’s a good chance to help people.\n",
            "Output order:  ['C', 'D', 'B', 'A']\n",
            "Question: Q20.  From the ad, which is true about Sunny Sun pictures? \n",
            "Answer: \n",
            " \t\t The first day to use Sunny Sun pictures is Feb. 4. \t\t Score: 0.0073\n",
            " \t\t The last day to collect Sunny Sun pictures is Feb. 10. \t\t Score: 0.0220\n",
            " \t\t You cannot use Sunny Sun pictures to get free desserts. \t\t Score: 0.0239\n",
            " \t\t You cannot get Sunny Sun pictures when you buy books.   \t\t Score: -0.0079\n",
            "(C)You cannot use Sunny Sun pictures to get free desserts.\n",
            "Output order:  ['C', 'B', 'A', 'D']\n",
            "Question: Q21. Sammy has 7 Sunny Sun pictures. She wants to get a Sunny Cup. How can she get one? \n",
            "Answer: Collect 12 pictures, and you can get a Sunny Cup for free\n",
            "Collect 12 pictures, and you can get a Sunny Cup for free \t\t Use 6 Sunny Sun pictures and pay $100. \t\t Score: 0.2968\n",
            "Collect 12 pictures, and you can get a Sunny Cup for free \t\t Use 6 Sunny Sun pictures and pay $200. \t\t Score: 0.3113\n",
            "Collect 12 pictures, and you can get a Sunny Cup for free \t\t Spend $150 to get 3 more Sunny Sun pictures. \t\t Score: 0.3187\n",
            "Collect 12 pictures, and you can get a Sunny Cup for free \t\t Spend $200 to get 4 more Sunny Sun pictures.    \t\t Score: 0.2538\n",
            "(C)Spend $150 to get 3 more Sunny Sun pictures.\n",
            "Output order:  ['C', 'B', 'A', 'D']\n",
            "Question: Q23. What does \"them\" mean in the reading? \n",
            "Answer: mix chocolate with butter and sugar\n",
            "mix chocolate with butter and sugar \t\t Bains-marie. \t\t Score: 0.0151\n",
            "mix chocolate with butter and sugar \t\t Butter and sugar. \t\t Score: 0.7233\n",
            "mix chocolate with butter and sugar \t\t Chocolate desserts. \t\t Score: 0.6677\n",
            "mix chocolate with butter and sugar \t\t Small pieces of chocolate.   \t\t Score: 0.5533\n",
            "(B)Butter and sugar.\n",
            "Output order:  ['B', 'C', 'D', 'A']\n",
            "Question: Q24. From the reading, which is true about working with chocolate? \n",
            "Answer: Easy\n",
            "Easy \t\t It is better to use a “bain-marie” than a “water bath.” \t\t Score: 0.1790\n",
            "Easy \t\t The water in the pot should not be warmer than 50ºC. \t\t Score: -0.0485\n",
            "Easy \t\t It is better to finish mixing it in less than five minutes. \t\t Score: 0.2252\n",
            "Easy \t\t We should not mix butter and sugar together at the same time.   \t\t Score: 0.0644\n",
            "(C)It is better to finish mixing it in less than five minutes.\n",
            "Output order:  ['C', 'A', 'D', 'B']\n",
            "Question: Q25. What does the news story say about the Yangs’ food truck? \n",
            "Answer: [CLS]\n",
            "[CLS] \t\t What people love about their food. \t\t Score: -0.0322\n",
            "[CLS] \t\t What made them start their business. \t\t Score: -0.0314\n",
            "[CLS] \t\t How they made delicious fried chicken. \t\t Score: -0.0785\n",
            "[CLS] \t\t How they fixed their business problems.   \t\t Score: -0.0644\n",
            "(B)What made them start their business.\n",
            "Output order:  ['B', 'A', 'D', 'C']\n",
            "Question: Q26. What does it mean when business is \"slack\"? \n",
            "Answer: People thought they were just another Chinese food truck\n",
            "People thought they were just another Chinese food truck \t\t It is bad. \t\t Score: 0.0415\n",
            "People thought they were just another Chinese food truck \t\t It is for sale \t\t Score: -0.0026\n",
            "People thought they were just another Chinese food truck \t\t It is growing. \t\t Score: 0.0259\n",
            "People thought they were just another Chinese food truck \t\t It is open every day.   \t\t Score: -0.0205\n",
            "(A)It is bad.\n",
            "Output order:  ['A', 'C', 'B', 'D']\n",
            "Question: Q27. What does \"that\" mean in the news story?  \n",
            "Answer: [CLS]\n",
            "[CLS] \t\t A Taiwanese restaurant. \t\t Score: 0.0961\n",
            "[CLS] \t\t A second Taiwanese food truck. \t\t Score: 0.1133\n",
            "[CLS] \t\t A city block that sells Taiwanese food. \t\t Score: 0.0766\n",
            "[CLS] \t\t A business that is popular across the country.   \t\t Score: 0.0724\n",
            "(B)A second Taiwanese food truck.\n",
            "Output order:  ['B', 'A', 'C', 'D']\n",
            "Question: Q28. What problem is Selena Bieber trying to fix? \n",
            "Answer: We ’ re asked to get to school by 7 : 30 a. m. That means we have to get up before 7, when our brains should still be sleeping\n",
            "We ’ re asked to get to school by 7 : 30 a. m. That means we have to get up before 7, when our brains should still be sleeping \t\t High school lessons are too difficult. \t\t Score: 0.1569\n",
            "We ’ re asked to get to school by 7 : 30 a. m. That means we have to get up before 7, when our brains should still be sleeping \t\t Classes start too early in the morning. \t\t Score: 0.5345\n",
            "We ’ re asked to get to school by 7 : 30 a. m. That means we have to get up before 7, when our brains should still be sleeping \t\t Too many students are late for school. \t\t Score: 0.4463\n",
            "We ’ re asked to get to school by 7 : 30 a. m. That means we have to get up before 7, when our brains should still be sleeping \t\t Lunch break is too short for taking a rest.    \t\t Score: 0.1243\n",
            "(B)Classes start too early in the morning.\n",
            "Output order:  ['B', 'C', 'A', 'D']\n",
            "Question: Q29. What can we learn from the letter? \n",
            "Answer: we can not only sleep more but also learn better\n",
            "we can not only sleep more but also learn better \t\t Too much melatonin can hurt teenagers’ brain. \t\t Score: 0.1891\n",
            "we can not only sleep more but also learn better \t\t The brain stops making melatonin after 11 p.m. \t\t Score: 0.2963\n",
            "we can not only sleep more but also learn better \t\t Sleeping longer helps the body make more melatonin. \t\t Score: 0.4024\n",
            "we can not only sleep more but also learn better \t\t It is easier to fall asleep when the brain is making melatonin.   \t\t Score: 0.4765\n",
            "(D)It is easier to fall asleep when the brain is making melatonin.  \n",
            "Output order:  ['D', 'C', 'B', 'A']\n",
            "Question: Q30. Below is some information from another study. We studied 48 high school students and found that the students who went to bed earlier did better on their school tests. This is not only true for those who slept nine hours, but also true for those who slept less. And some of them agreed that they learned better in early morning classes. Do the ideas in Selena Bieber’s letter agree with the information? \n",
            "Answer: [CLS]\n",
            "[CLS] \t\t Yes, because her letter says it is better to sleep from 11 p.m. to 8 a.m. \t\t Score: 0.0751\n",
            "[CLS] \t\t Yes, because her letter says getting up early is not helpful for learning. \t\t Score: 0.0339\n",
            "[CLS] \t\t No, because her letter says students are asked to get to school by 7:30 a.m. \t\t Score: -0.0279\n",
            "[CLS] \t\t No, because her lettersaysteenagers’ brains do not work well in early morning classes.   \t\t Score: 0.0860\n",
            "(D)No, because her lettersaysteenagers’ brains do not work well in early morning classes.  \n",
            "Output order:  ['D', 'A', 'B', 'C']\n",
            "Question: Q31. What can we learn about Kivalina? \n",
            "Answer: It cannot be found on most maps of Alaska because it is only 10 km2. This small Arctic village is home to 400 Inuit people\n",
            "It cannot be found on most maps of Alaska because it is only 10 km2. This small Arctic village is home to 400 Inuit people \t\t It is waiting to shine. \t\t Score: -0.0360\n",
            "It cannot be found on most maps of Alaska because it is only 10 km2. This small Arctic village is home to 400 Inuit people \t\t It has become history. \t\t Score: 0.0640\n",
            "It cannot be found on most maps of Alaska because it is only 10 km2. This small Arctic village is home to 400 Inuit people \t\t It is fighting for one last hope. \t\t Score: 0.0574\n",
            "It cannot be found on most maps of Alaska because it is only 10 km2. This small Arctic village is home to 400 Inuit people \t\t It has given up its chance to rise.   \t\t Score: -0.0255\n",
            "(B)It has become history.\n",
            "Output order:  ['B', 'C', 'D', 'A']\n",
            "Question: Q32. What does it mean when we say a place is uninhabitable? \n",
            "Answer: the island could be covered by the Chukchi Sea\n",
            "the island could be covered by the Chukchi Sea \t\t It is not big. \t\t Score: 0.1138\n",
            "the island could be covered by the Chukchi Sea \t\t It is not popular. \t\t Score: 0.0243\n",
            "the island could be covered by the Chukchi Sea \t\t It cannot be bought. \t\t Score: 0.0897\n",
            "the island could be covered by the Chukchi Sea \t\t It cannot be lived in.   \t\t Score: 0.1451\n",
            "(D)It cannot be lived in.  \n",
            "Output order:  ['D', 'A', 'C', 'B']\n",
            "Question: Q33. What does the writer think about the Inuit people in Kivalina?  \n",
            "Answer: [CLS]\n",
            "[CLS] \t\t Their way of living has been hurting the earth. \t\t Score: 0.0751\n",
            "[CLS] \t\t They might not be able to get enough money to move their village. \t\t Score: 0.0818\n",
            "[CLS] \t\t They should make the oil and power companies fix their problems. \t\t Score: -0.0715\n",
            "[CLS] \t\t There are better ways than to move their village to somewhere else.   \t\t Score: 0.0053\n",
            "(B)They might not be able to get enough money to move their village.\n",
            "Output order:  ['B', 'A', 'D', 'C']\n",
            "Question: Q34. What does the reading say about the Arctic ice? \n",
            "Answer: the Arctic ice has kept melting\n",
            "the Arctic ice has kept melting \t\t The Arctic ice covered 10 km2 of Kivalina. \t\t Score: 0.2897\n",
            "the Arctic ice has kept melting \t\t The melting Arctic ice kills about 400 Inuit people each year. \t\t Score: 0.3803\n",
            "the Arctic ice has kept melting \t\t The Arctic ice helped the Inuit people in Kivalina during bad weather. \t\t Score: 0.5250\n",
            "the Arctic ice has kept melting \t\t The melting Arctic ice has given the Inuit people enough water to use.                 \t\t Score: 0.4696\n",
            "(C)The Arctic ice helped the Inuit people in Kivalina during bad weather.\n",
            "Output order:  ['C', 'D', 'B', 'A']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}